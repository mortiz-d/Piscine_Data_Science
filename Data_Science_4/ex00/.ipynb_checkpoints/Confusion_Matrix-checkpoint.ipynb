{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix \t\t#Usar para comprobar resultados\n",
    "from sklearn.metrics import classification_report \t#Usar para comprobar resultados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_pred = \"../predictions.txt\"\n",
    "direction_truth = \"../truth.txt\"\n",
    "\n",
    "yhat = pd.read_csv(direction_pred, header=None, names=[\"Rol\"])[\"Rol\"].map({\"Jedi\": 1, \"Sith\": 0})\n",
    "y = pd.read_csv(direction_truth, header=None, names=[\"Rol\"])[\"Rol\"].map({\"Jedi\": 1, \"Sith\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos nuestras funciones de matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui contamos el porcentaje de fallos y aciertos de las predicciones(yhat) vs el resultado real(y) \n",
    "def calculate_matrix(y,y_predict):\n",
    "    matrix = [[0,0],\n",
    "              [0,0]]\n",
    "    if y.size != y_predict.size:\n",
    "        print(\"Error : y and yhat dont have the same size \")\n",
    "        return matrix\n",
    "    for index in range(y.size):\n",
    "        if y.iloc[index] == y_predict.iloc[index] and y.iloc[index] == 1: #True positive\n",
    "            matrix[0][0] = matrix[0][0] + 1\n",
    "        elif y.iloc[index] != y_predict.iloc[index] and y_predict.iloc[index] == 1: # False positive\n",
    "            matrix[1][0] = matrix[1][0] + 1\n",
    "        elif y.iloc[index] == y_predict.iloc[index] and y.iloc[index] == 0: # True negative\n",
    "            matrix[1][1] = matrix[1][1] + 1\n",
    "        elif y.iloc[index] != y_predict.iloc[index] and y_predict.iloc[index] == 0: # False negative\n",
    "            matrix[0][1] = matrix[0][1] + 1\n",
    "        else:\n",
    "            print (\"Encontrado algo que no es echale el ojo\")\n",
    "    return matrix\n",
    "\n",
    "\n",
    "#Calculamos las metricas\n",
    "def calculate_metrics(matrix):\n",
    "    TP = matrix[0][0] #True Positive\n",
    "    FN = matrix[0][1] #False Negative\n",
    "    FP = matrix[1][0] #False Positive\n",
    "    TN = matrix[1][1] #True Negatve\n",
    "\n",
    "    #Metrics for jedi\n",
    "    precision_j = TP / (TP + FP) #precision para determinar un jedi\n",
    "    recall_j = TP / (TP + FN) #sensibilidad para determinar un jedi\n",
    "    f1_score_j = (2 * (precision_j * recall_j)) / (precision_j + recall_j)\n",
    "    total_j = TP + FN #Total of jedis\n",
    "\n",
    "    #Shit (i didnt meant sith) Metrics\n",
    "    precision_s = TN / (TN + FN) #precision para determinar un sith\n",
    "    recall_s = TN / (TN + FP) #sensibilidad para determinar un sith\n",
    "    f1_score_s = (2 * (precision_s * recall_s)) / (precision_s + recall_s)\n",
    "    total_s = TN + FP #Total of shits\n",
    "\n",
    "\t#accuracy over total predictions\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    total = TP + TN + FP + FN\n",
    "\n",
    "    print(f\"{'':<10}{'precision':>10}{'recall':>10}{'f1-score':>12}{'total':>8}\")\n",
    "    print(f\"{'Jedi : 1':<10}{precision_j:>10.2f}{recall_j:>10.2f}{f1_score_j:>12.2f}{total_j:>8}\")\n",
    "    print(f\"{'Sith : 0':<10}{precision_s:>10.2f}{recall_s:>10.2f}{f1_score_s:>12.2f}{total_s:>8}\")\n",
    "    print(f\"{'accuracy':<30}{accuracy:>12.2f}{total:>8}\")\n",
    "    return  \n",
    "\n",
    "\n",
    "#       Confusion Table\n",
    "#           Predicci√≥n\n",
    "#           1     0\n",
    "#Real  1 | TP | FN |\n",
    "#      0 | FP | TN |\n",
    "#Funcion para el plot\n",
    "def plot_confusion_matrix(matrix):\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(matrix, annot=True, ax = ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Jedi : 1', 'Sith : 0']); ax.yaxis.set_ticklabels(['Jedi : 1', 'Sith : 0'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#\"this function plots the confusion matrix using metrics\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "matrix = calculate_matrix(y,yhat)\n",
    "calculate_metrics(matrix)\n",
    "plot_confusion_matrix(matrix)\n",
    "\n",
    "#Comprobacion contra las metricas de SKLEARN\n",
    "#print(\"- - - - - - - - - - - - - - - - - - - - - -\")\n",
    "#matrix = confusion_matrix(y,yhat)\n",
    "#print(classification_report(y, yhat))\n",
    "#plot_confusion_matrix(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
